\section{Introduction}
Hyperbolic equations play an important role in various research areas such as fluid dynamics or plasma physics. Efficient numerical methods combined with robust implementations are available for these problems, however they do not account for uncertainties which can arise in measurement data or modeling assumptions. Including the effects of uncertainties in differential equations has become an important topic in the last decades. %Examples include computational fluid dynamics \cite{bijl2013uncertainty}. 

A general hyperbolic set of equations with random initial data can be written as
\begin{subequations}\label{eq:hyperbolicProblem}
\begin{align}
\partial_t \bm{u}(t,\bm{x},\bm{\xi}) + \nabla&\cdot\bm{f}(\bm{u}(t,\bm{x},\bm{\xi})) = \bm{0} \enskip \text{ in } D, \\ \label{eq:ic}
\bm{u}(t=0,\bm{x},&\bm{\xi}) = \bm{u}_{\text{IC}}(\bm{x},\bm{\xi}),
\end{align}
\end{subequations}
where the solution $\bm u\in\mathbb{R}^p$ depends on time $t\in\mathbb{R}^+$, spatial position $\bm{x}\in D\subseteq \mathbb{R}^d$ as well as a vector of random variables $\bm{\xi}\in\Theta\subseteq\mathbb{R}^s$ with given probability density functions $f_{\Xi,i}(\xi_i)$ for $i = 1,\cdots,s$. Hence, the probability density function of $\bm{\xi}$ is then given by $f_{\Xi}(\bm\xi):=\prod_{i=1}^s f_{\Xi,i}(\xi_i)$. The physical flux is given by $\bm{f}:\mathbb{R}^p\to\mathbb{R}^{d\times p}$. To simplify notation, we assume that only the initial condition is random, i.e. $\bm{\xi}$ enters through the definition of $\bm{u}_{IC}$. Equations \eqref{eq:hyperbolicProblem} are usually supplemented with boundary conditions, which we will specify later for the individual problems.

Due to the randomness of the solution, one is interested in determining the expectation value or the variance, i.e.
\begin{align*}
\text{E}[\bm{u}] = \langle \bm{u} \rangle,\qquad \text{Var}[\bm{u}] = \langle \left( \bm{u}-\text{E}[\bm{u}]\right)^2\rangle,
\end{align*}
where we use the bracket operator $\langle \cdot \rangle := \int_{\Theta} \cdot f_{\Xi}(\bm\xi)d\xi_1 \cdots d\xi_s$. More generally, one is interested in determining the moments of the solution for a given set of basis functions $\varphi_{i}:\Theta\to\mathbb{R}$ such that for the multi-index $i = (i_1,\cdots,i_s)$ we have $|i| \leq M$. Commonly one chooses orthonormal polynomials as basis functions \cite{xiu2002wiener}, i.e. $\langle \varphi_n \varphi_m \rangle = \delta_{nm}$.  The moments are then given by $\bm{\hat u_i} := \langle \bm{u}\varphi_i \rangle$. Besides facilitating the computation of the expectation value and variance, the moments can be used to span the solution in $\bm\xi$ \cite{wiener1938homogeneous}. \\

Numerical methods for approximating the moment $\bm{\hat u_i}$ can be divided into intrusive and non-intrusive techniques. A popular non-intrusive method is the stochastic-Collocation (SC) method, see e.g. \cite{xiu2005high,babuvska2007stochastic,loeven2008probabilistic}, which computes the moments with the help of a numerical quadrature rule: For a given set of $N_q$ quadrature weights $w_k$ and quadrature points $\bm{\xi}_k$, the moments are approximated by
\begin{align*}
\bm{\hat u_i} = \langle \bm{u}\varphi_i \rangle \approx \sum_{k = 1}^{N_q}w_k \bm{u}({t,\bm{x},\bm{\xi}_k})\varphi_i(\bm{\xi}_k)f_{\Xi}(\bm{\xi}_k).
\end{align*} 
Since the solution at a fixed quadrature point can be computed by a standard deterministic solver, the SC method does not require a significant implementation effort. Furthermore, SC is embarrassingly parallel, since the required computations can be carried out in parallel on different cores. A downside of collocation methods are aliasing effects, which stem from the inexact approximation of integrals. Furthermore, collocation methods require more runs of the deterministic solver than intrusive methods \cite{xiu2009fast,alekseev2011estimation}. Despite their easy implementation, collocation methods face challenges when examining unsteady problems, since in this case, the solution needs to be written out, i.e. stored in an external file, at all quadrature points in every time step to compute the time evolution of the moments. This contradicts modern HPC paradigms, which aim at reducing the amount of data produced by numerical methods.

Intrusive methods do not suffer from this problem, since the computation is directly carried out on the moments, i.e. their time evolution can be recorded during the computation. Also for steady problems, the fact that the moments are known in each iteration enables the computation of the stochastic residual, which indicates when to stop the iteration towards the steady state solution. However intrusive methods are in general more difficult to implement and come along with higher numerical costs. The main idea of these methods is to derive a system of equations for the moments and then implementing a numerical solver for this system: Testing the initial problem \eqref{eq:hyperbolicProblem} with $\varphi_i$ for $|i|\leq M$ yields
\begin{subequations}\label{eq:nonClosedMomentSystem}
\begin{align}
\partial_t \bm{\hat u}_i(t,\bm{x}) + \nabla&\cdot\langle\bm{f}(\bm{u}(t,\bm{x},\cdot)) \varphi_i\rangle = \bm{0}, \\
\bm{\hat u_i}(t=0,\bm{x}&) = \langle\bm{u}_{\text{IC}}(\bm{x},\cdot)\varphi_i\rangle.
\end{align}
\end{subequations}
To obtain a closed set of equations, one needs to derive a closure $\mathcal{U}$ such that 
\begin{align*}
\bm{u}(t,\bm x,\bm \xi) \approx \mathcal{U}(\bm{\hat u}_0,\cdots,\bm{\hat u}_N;\bm\xi).
\end{align*}
A commonly used closure is given by stochastic-Galerkin (SG) \cite{ghanem2003stochastic}, which represents the solution by a polynomial:
\begin{align*}
\mathcal{U}_{\text{SG}}(\bm{\hat u}_0,\cdots,\bm{\hat u}_N;\bm\xi):= \sum_{i=0}^N \bm{\hat{u}}_i\varphi_i(\bm{\xi}) = \hat{\bm u}^T\bm{\varphi}(\bm\xi),
\end{align*}
Here, we collect the moments for which $\vert i \vert \leq M$ holds in the moment matrix $\bm{\hat{u}}:=(\bm{\hat{u}}_0,\cdots,\bm{\hat{u}}_N)^T\in\mathbb{R}^{(N+1) \times p}$ and the corresponding basis functions in $\bm{\varphi}:=(\varphi_0,\cdots,\varphi_N)^T\in\mathbb{R}^{N+1}$. When using the stochastic-Galerkin method to close \eqref{eq:nonClosedMomentSystem}, the resulting moment system is not necessarily hyperbolic \cite{poette2009uncertainty} and the solution needs to be manipulated \cite{schlachter2018hyperbolicity} in order to prevent a failure of the method. A generalization of stochastic-Galerkin, which ensures hyperbolicity is the Intrusive Polynomial Moment (IPM) closure \cite{poette2009uncertainty}: The closure is given by a constraint optimization problem. For a given convex entropy $s:\mathbb{R}^p\to\mathbb{R}$ for the original problem \eqref{eq:hyperbolicProblem}, this optimization problem is given by
\begin{align}\label{eq:primalProblem}
\mathcal{U}(\bm{\hat u}) = \argmin_{\bm u} \langle s(\bm u) \rangle \enskip \text{ subject to } \bm{\hat u}_i = \langle \bm u \varphi_i \rangle \text{ for } i = 0,\cdots,N.
\end{align}
Rewritten in its dual form, \eqref{eq:primalProblem} is transformed into an unconstraint optimization problem, given by
\begin{align}\label{eq:dualProblem}
 \bm{\hat \lambda}(\bm{\hat u}) := \argmin_{\bm{\lambda} \in \mathbb{R}^{(N+1) \times p}}
  \left\{\langle s_*(\bm{\lambda}^T \bm\varphi)\rangle - \sum_{i=0}^{N}\bm{\lambda}_i^T \bm{\hat u}_i\right\},
\end{align}
where $s_*:\mathbb{R}^p\to\mathbb{R}$ is the Legendre transformation of $s$, and $\bm{ \hat\lambda}:=(\bm{\hat{\lambda}}_0,\cdots,\bm{\hat{\lambda}}_N)^T\in \mathbb{R}^{(N+1) \times p}$ are called the dual variables. The solution to \eqref{eq:primalProblem} is then given by
\begin{align}\label{eq:ansatz}
 \mathcal{U}(\bm{\hat u}) = \left( \nabla_{\bm{u}} s \right)^{-1}(\bm{\hat{\lambda}}(\bm{\hat u})^T \bm{\varphi}).
\end{align}
Plugging the derived closure into the moment system \eqref{eq:nonClosedMomentSystem}, one obtains the IPM moment system
\begin{subequations}\label{eq:IPMmomentSystem}
\begin{align}
\partial_t \bm{\hat u}_i(t,\bm{x}) + \nabla&\cdot\langle\bm{f}(\mathcal{U}(\bm{\hat u})) \varphi_i\rangle = \bm{0}, \\
\bm{\hat u_i}(t=0,\bm{x}&) = \langle\bm{u}_{\text{IC}}(\bm{x},\cdot)\varphi_i\rangle.
\end{align}
\end{subequations}
with $|i|\leq M$. The IPM method has several advantages: Choosing the entropy $s(\bm{u}) = \frac{1}{2}\bm{u}^T\bm{u}$ yields the stochastic-Galerkin closure, i.e. IPM generalizes different intrusive methods. Furthermore, at least for scalar problems, IPM is significantly less oscillatory compared to SG \cite{kusch2017maximum}. Also, as discussed in \cite{poette2009uncertainty}, when choosing $s(\bm u)$ to be a physically correct entropy of the deterministic problem, the IPM solution dissipates the expectation value of the entropy, which is
\begin{align*}
S(\bm{\hat u}) := \langle s( \mathcal{U}(\bm{\hat u}))\rangle,
\end{align*}
i.e. the IPM method yields a physically correct entropy solution. This again underlines a weakness of stochastic-Galerkin: If $s(\bm{u}) = \frac{1}{2}\bm{u}^T\bm{u}$ is not a correct entropy of the original problem, the SG method can lead to non-physical solution values, which can then cause a failure of the method. The main weakness of the IPM method is its run time, since it requires the repeated evaluation of \eqref{eq:ansatz}, which involves solving the optimization problem \eqref{eq:dualProblem}. Hence, the desirable properties of IPM come along with significantly increased numerical costs. However, IPM and minimal entropy methods in general are well suited for modern HPC architecture, which can be used to reduce the run time \cite{garrett2015optimization}. 

When studying hyperbolic equations, the moment approximations of various methods such as Stochastic Galerkin \cite{le2004uncertainty}, IPM \cite{kusch2018filtered} and stochastic-Collocation \cite{barth2013non,dwight2013adaptive} tend to show incorrect discontinuities in certain regions of the physical space. These non-physical structures tend to dissolve when the number of basis functions is increased \cite{pettersson2009numerical,offner2017stability} or when artificial diffusion is added through the spatial numerical method \cite{offner2017stability} or filters \cite{kusch2018filtered}. Here, intrusive methods seem to be an adequate choice since they are well suited for adaptive strategies which locally increase the polynomial order \cite{tryoen2012adaptive,kroker2012finite,giesselmann2017posteriori} or add artificial viscosity \cite{kusch2018filtered} at certain spatial positions and time steps in which complex structures such as discontinuities occur. The main task here is to find an adequate refinement indicator. \\

In this paper, we present a semi-intrusive code framework, which facilitates the task of implementing general intrusive methods. The framework only requires the numerical flux of the deterministic problem (as well as an entropy if IPM is used). We thereby provide the ability to recycle existing implementations of deterministic solvers.
Furthermore, we investigate intrusive methods for steady problems and compare them to collocation methods. The steady setting provides different opportunities to take advantage of features of intrusive methods: 
\begin{itemize}
\item Accelerate convergence to the IPM steady state solution by applying IPM as a post-processing step for collocation methods: We converge the moments of the solution to a steady state with an inaccurate, but cheap collocation method and then use the resulting collocation moments as starting values for an expensive but accurate intrusive method such as IPM, which we then again converge to steady state. 
\item Compute inexact dual variables \eqref{eq:dualProblem} for IPM: Since the moments during the iteration process are inaccurate, i.e. they are not the correct steady state solution, we propose to not fully converge the dual iteration, which computes \eqref{eq:dualProblem}. 
\end{itemize}
In addition to these two strategies for steady problems, we make us of adaptivity: The intrusive nature of SG and IPM can be used to locally increase the number of moments whenever the solution has a complex structure in the random variable (as well as decrease the number of moments if not). We test the effectiveness of these acceleration ideas by comparing results with stochastic-Collocation for the uncertain NACA test case. 

The paper is structured as follows: After the introduction, we discuss the numerical discretization as well as the implementation and structure of the semi-intrusive framework in section \ref{sec:framework}. Section \ref{sec:collIPM} discusses the IPM acceleration with a non-intrusive method and in section \ref{sec:OneShotIPM}, we discuss the idea of not converging the dual iteration. A comparison of results computed with the presented methods is given in \ref{sec:results}, followed by a summary and outlook in section \ref{sec:summary_outlook}.

