\section{Introduction}
Hyperbolic equations play an important role in various research areas such as fluid dynamics or plasma physics. Efficient numerical methods combined with robust implementations are widely available for these problems, however they do not account for uncertainties which can arise in measurement data or modeling assumptions. Including the effects of uncertainties in differential equations has become an important topic in the last decades. %Examples include computational fluid dynamics \cite{bijl2013uncertainty}. 

One strategy to represent the solution's dependence on uncertainties is to use a polynomial chaos (PC) expansion \cite{wiener1938homogeneous,xiu2002wiener}, i.e. the uncertainty space is spanned by polynomial basis functions. The remaining task is then to determine adequate expansion coefficients, often called moments or PC coefficients. Numerical methods for approximating these coefficients can be divided into intrusive and non-intrusive techniques. A popular non-intrusive method is the stochastic-Collocation (SC) method, see e.g. \cite{xiu2005high,babuvska2007stochastic,loeven2008probabilistic}, which computes the moments with the help of a numerical quadrature rule. Commonly, SC uses sparse grids, since they posses a reduced number of collocation points for multi-dimensional problems. Since the solution at a fixed quadrature point can be computed by a standard deterministic solver, the SC method does not require a significant implementation effort. Furthermore, SC is embarrassingly parallel, since the required computations decouple and the workload can easily be distributed across several processors. 

The main idea of intrusive methods is to derive a system of equations describing the time evolution of the moments which can then be solved with a deterministic numerical scheme. A popular approach to describe the moment system is the stochastic-Galerkin (SG) method \cite{ghanem2003stochastic}, which chooses a polynomial basis ansatz of the solution and performs a Galerkin projection to derive a closed system of equations. One significant drawback of SG is, that its moment system is not necessarily hyperbolic \cite{poette2009uncertainty}. A generalization of stochastic-Galerkin, which ensures hyperbolicity is the Intrusive Polynomial Moment (IPM) method \cite{poette2009uncertainty}. Instead of performing the PC expansion on the solution, the IPM method represents the entropy variables with polynomials. Besides yielding a hyperbolic moment system, the IPM method has several advantages: Choosing a quadratic entropy yields the stochastic-Galerkin moment system, i.e. IPM generalizes different intrusive methods. Furthermore, at least for scalar problems, IPM is significantly less oscillatory compared to SG \cite{kusch2017maximum}. Also, as discussed in \cite{poette2009uncertainty}, when choosing a physically correct entropy of the deterministic problem, the IPM solution dissipates the expectation value of the entropy, i.e. the IPM method yields a physically correct entropy solution. Unfortunately, the desirable properties of IPM come along with significantly increased numerical costs, since IPM requires the repeated computation of the entropic expansion coefficients from the moment vector, which involves solving a convex optimization problem. However, IPM and minimal entropy methods in general are well suited for modern HPC architectures \cite{garrett2015optimization}. 

When studying hyperbolic equations, the moment approximations of various methods such as Stochastic Galerkin \cite{le2004uncertainty,kusch2018filtered}, IPM \cite{kusch2018filtered,poette2019contribution} and stochastic-Collocation \cite{barth2013non,dwight2013adaptive} tend to show incorrect discontinuities in certain regions of the physical space. These non-physical structures dissolve when the number of basis functions is increased \cite{pettersson2009numerical,offner2017stability} or when artificial diffusion is added through either the spatial numerical method \cite{offner2017stability} or by filters \cite{kusch2018filtered}. Also, a multi-element approach which divides the uncertain domain into cells and uses piece-wise polynomial basis functions to represent the solution has proven to mitigate non-physical discontinuities \cite{wan2006multi,durrwachter2018hyperbolicity}. Non-intrusive Monte-Carlo methods \cite{mishra2012multi,mishra2012sparse,mishra2016numerical}, which randomly sample input unceratinties to compute quantities of interest are robust, but suffer from a slow rate of convergence while again lacking the ability to use adaptivity to their full extend. Discontinuous structures commonly arise on a small portion of the space-time domain. Therefore, intrusive methods seem to be an adequate choice since they are well suited for adaptive strategies. By locally increasing the polynomial order \cite{tryoen2012adaptive,kroker2012finite,giesselmann2017posteriori} or adding artificial viscosity \cite{kusch2018filtered} at certain spatial positions and time steps in which complex structures such as discontinuities occur, a given accuracy can be reached with significantly reduced numerical costs. In addition to that, the number of moments needed to obtain a certain order with intrusive methods is smaller than the number of quadrature points for SC. An additional downside of collocation methods are aliasing effects, which stem from the inexact approximation of integrals. Consequently, collocation methods typically require a higher number of unknowns than intrusive methods to reach a given accuracy \cite{xiu2009fast,alekseev2011estimation}. Therefore, one aim should be to accelerate intrusive methods, since they can potentially outperform non-intrusive methods in complex and high-dimensional settings. \\

In this paper, we propose acceleration techniques for intrusive methods and compare them against stochastic-Collocation. For steady and unsteady problems, we use adaptivity, for which intrusive methods provide a convenient framework: %Since complex structures in the uncertain domain tend to arise in small portions of the spatial mesh, our aim is to locally increase the accuracy of the stochastic discretization in region that show a complex structure in the random domain, while choosing a low order method in the remainder:
\begin{itemize}
\item Since complex structures in the uncertain domain tend to arise in small portions of the spatial mesh, our aim is to locally increase the accuracy of the stochastic discretization in region that show a complex structure in the random domain, while choosing a low order method in the remainder. Such an adaptive treatment cannot be realized with non--intrusive methods, since one needs to break up the black-box approach. To guarantee an efficient implementation, we propose an adaptive discretization strategy for IPM.
\end{itemize}
A steady problem provides different opportunities to take advantage of features of intrusive methods: 
\begin{itemize}
\item When using adaptivity, one can perform a large number of iterations to the steady state solution on a low number of moments and increase the maximal truncation order when the distance to the steady state has reached a specified barrier. Consequently, a large number of iterations will be performed by a cheap, low order method, i.e. we can reduce numerical costs. 
\item Perform an inexact map from the moments to the entropic expansion coefficients for IPM: Since the moments during the iteration process are inaccurate, i.e. they are not the correct steady state solution, we propose to not fully converge the dual iteration, which solves the IPM optimization problem. Consequently, the entropic expansion coefficients and the moments are converged simultaneously to their steady state, which is similar to the idea of One-Shot optimization in shape optimization \cite{hazra2005aerodynamic}.
\end{itemize}

The effectiveness of these acceleration ideas are tested by comparing results with stochastic-Collocation for the uncertain NACA test case as well as a bent shock tube problem. Our numerical studies show the following main results:
\begin{itemize}
\item In our test cases, the need to solve an optimization problem when using the IPM method leads to a significantly higher run time than SC and SG. However when using the discussed acceleration techniques, IPM requires the shortest time to reach a given accuracy.
\item Comparing SG with IPM, one observes that for the same number of unknowns, SG yields more accurate expectation values, whereas IPM shows improved variance approximations.
\item By studying aliasing effects, we show that SC requires a higher number of unknowns than intrusive methods (even for a one-dimensional uncertainty) to reach the same accuracy level.
\item Using sparse grids for the IPM discretization when the space of uncertainty is multi-dimensional, the number of quadrature points needed to guarantee sufficient regularity of the Hessian matrix is significantly increased.
\end{itemize}
The IPM and SG calculations use a semi-intrusive numerical method, meaning that the discretization allows recycling a given deterministic code to generate the IPM solver. While facilitating the task of implementing general intrusive methods, this framework reduces the number of operations required to compute numerical fluxes. Also, it provides the ability to base the intrusive method on the same deterministic solver as used in the implementation of a black-box fashion stochastic-Collocation code, which allows for, what we believe, a fair comparison between intrusive and non-intrusive methods. The code is publicly available to allow reproducibility \cite{uqcreator}.

The paper is structured as follows: After the introduction, we present the discussed methods in more detail in section~\ref{sec:background}. The numerical discretization as well as the implementation and structure of the semi-intrusive method is introduced in section \ref{sec:framework}. In section~\ref{sec:OneShotIPM}, we discuss the idea of not converging the dual iteration. Section~\ref{sec:adaptivity} extends the presented numerical framework to an algorithm making use of adaptivity. Implementation and parallelization details are given in section~\ref{sec:parallel}. A comparison of results computed with the presented methods is then given in section \ref{sec:results}, followed by a summary and outlook in section \ref{sec:summary_outlook}.

\section{Background}
\label{sec:background}
In the following, we briefly introduce the notation and methods used in this work. A general hyperbolic set of equations with random initial data can be written as
\begin{subequations}\label{eq:hyperbolicProblem}
\begin{align}
\label{eq:fulleq}\partial_t \bm{u}(t,\bm{x},\bm{\xi}) + \nabla&\cdot\bm{f}(\bm{u}(t,\bm{x},\bm{\xi})) = \bm{0} \enskip \text{ in } D, \\ \label{eq:ic}
\bm{u}(t=0,\bm{x},&\bm{\xi}) = \bm{u}_{\text{IC}}(\bm{x},\bm{\xi}),
\end{align}
\end{subequations}
where the solution $\bm u\in\mathbb{R}^m$ depends on time $t\in\mathbb{R}^+$, spatial position $\bm{x}\in D\subseteq \mathbb{R}^d$ as well as a vector of random variables $\bm{\xi}\in\Theta\subseteq\mathbb{R}^p$ with given probability density functions $f_{\Xi,i}(\xi_i)$ for $i = 1,\cdots,p$. Hence, the probability density function of $\bm{\xi}$ is $f_{\Xi}(\bm\xi):=\prod_{i=1}^p f_{\Xi,i}(\xi_i)$. The physical flux is given by $\bm{f}:\mathbb{R}^m\to\mathbb{R}^{d\times m}$. To simplify notation, we assume that only the initial condition is random, i.e. $\bm{\xi}$ enters through the definition of $\bm{u}_{IC}$. Equations \eqref{eq:hyperbolicProblem} are usually supplemented with boundary conditions, which we will specify later for the individual problems.

Due to the randomness of the solution, one is interested in determining the expectation value or the variance, i.e.
\begin{align*}
\text{E}[\bm{u}] = \langle \bm{u} \rangle,\qquad \text{Var}[\bm{u}] = \langle \left( \bm{u}-\text{E}[\bm{u}]\right)^2\rangle,
\end{align*}
where we use the bracket operator $\langle \cdot \rangle := \int_{\Theta} \cdot f_{\Xi}(\bm\xi)d\xi_1 \cdots d\xi_p$. To approximate quantities of interest (such as expectation value, variance or higher order moments), the solution is spanned with a set of polynomial basis functions $\varphi_{i}:\Theta\to\mathbb{R}$ such that for the multi-index $i = (i_1,\cdots,i_p)$ we have $|i| \leq M$. Note that this yields
\begin{align}\label{eq:numberBasisFcts}
N:=\begin{pmatrix}
M+p \\ p
\end{pmatrix}
\end{align}
basis functions when defining $|i|:=\sum_{n = 1}^p |i_n|$. Commonly, these functions are chosen to be orthonormal polynomials \cite{wiener1938homogeneous} with respect to the probability function, i.e. $\langle \varphi_i \varphi_j \rangle =\prod_{n=1}^p\delta_{i_nj_n}$. The generalized polynomial chaos (gPC) expansion \cite{xiu2002wiener} approximates the solution by
\begin{align}\label{eq:SGClosure}
\mathcal{U}(\bm{\hat u};\bm\xi):= \sum_{|i|\leq M} \bm{\hat{u}}_i\varphi_i(\bm{\xi}) = \hat{\bm u}^T\bm{\varphi}(\bm\xi),
\end{align}
where the deterministic expansion coefficients $\bm{\hat{u}}_i\in\mathbb{R}^m$ are called moments. To allow a more compact notation, we collect the $N$ moments for which $\vert i \vert \leq M$ holds in the moment matrix $\hat{\bm u}:=(\bm{\hat{u}}_i)_{|i|\leq M}\in\mathbb{R}^{N\times m}$ and the corresponding basis functions in $\bm{\varphi}:=(\varphi_i)_{|i|\leq M}\in\mathbb{R}^{N}$. In the following, the dependency of $\mathcal{U}$ on $\bm \xi$ will occasionally be omitted for sake of readability. The solution ansatz \eqref{eq:SGClosure} is $L^2$-optimal, if the moments are chosen to be the Fourier coefficients $\bm{\hat u}_i \equiv \langle \bm{u}\varphi_i \rangle\in\mathbb{R}^m$. One can also use the ansatz \eqref{eq:SGClosure} to compute the quantities of interest as
\begin{align*}
\text{E}[\mathcal{U}(\bm{\hat u})] = \bm{\hat u}_0,\quad \text{Var}[\mathcal{U}(\bm{\hat u})] = \text{E}[\mathcal{U}(\bm{\hat u})^2] - \text{E}[\mathcal{U}(\bm{\hat u})]^2 = \left(\sum_{i = 1}^N \hat{u}_{\ell i}^2\right)_{\ell = 1,\cdots,m}.
\end{align*}

The core idea of the stochastic-Collocation method is to compute the moments in the gPC expansion with a quadrature rule. Given a set of $Q$ quadrature weights $w_k$ and quadrature points $\bm{\xi}_k$, the moments are approximated by
\begin{align*}
\bm{\hat u}_i = \langle \bm{u}\varphi_i \rangle \approx \sum_{k = 1}^{Q}w_k \bm{u}({t,\bm{x},\bm{\xi}_k})\varphi_i(\bm{\xi}_k)f_{\Xi}(\bm{\xi}_k).
\end{align*} 
Hence, the moments can be computed by running a given deterministic solver for the original problem at each quadrature point. To reduce numerical costs in multi-dimensional settings, SC commonly uses sparse grids as quadrature rule: While tensorized quadrature sets require $O(M^p)$ quadrature points to integrate polynomials of maximal degree $M$ exactly, sparse grids are designed to integrate polynomials of total degree $M$, for which they only require $O(M(\log_2(M)^{p-1}))$ quadrature points. \remja{Brauchen wir hier noch ne Quelle?}

Intrusive methods derive a system which directly describes the time evolution of the moments: Plugging the solution ansatz \eqref{eq:SGClosure} into the set of equations \eqref{eq:hyperbolicProblem} and projecting the resulting residual to zero yields the stochastic-Galerkin moment system
\begin{subequations}\label{eq:SGMomentSystem}
\begin{align}
\partial_t \bm{\hat u}_i(t,\bm{x}) + \nabla&\cdot\langle\bm{f}(\mathcal{U}(\bm{\hat u})) \varphi_i\rangle = \bm{0}, \\
\bm{\hat u}_i(t=0,\bm{x}&) = \langle\bm{u}_{\text{IC}}(\bm{x},\cdot)\varphi_i\rangle,
\end{align}
\end{subequations}
with $|i|\leq M$. As already mentioned, the SG moment system is not necessarily hyperbolic. To ensure hyperbolicity, the IPM method uses a solution ansatz which minimizes a given entropy under a moment constraint instead of a polynomial expansion \eqref{eq:SGClosure}. For a given convex entropy $s:\mathbb{R}^m\to\mathbb{R}$ for the original problem \eqref{eq:hyperbolicProblem}, the IPM solution ansatz is given by
\begin{align}\label{eq:primalProblem}
\mathcal{U}(\bm{\hat u}) = \argmin_{\bm u} \langle s(\bm u) \rangle \enskip \text{ subject to } \bm{\hat u}_i = \langle \bm u \varphi_i \rangle \text{ for } |i| \leq M.
\end{align}
Rewritten in its dual form, \eqref{eq:primalProblem} is transformed into an unconstrained optimization problem. Defining the variables $\bm{\lambda}_i\in\mathbb{R}^m$, where $i$ is again a multi index, gives the unconstrained dual problem
\begin{align}\label{eq:dualProblem}
 \bm{\hat \lambda}(\bm{\hat u}) := \argmin_{\bm{\lambda} \in \mathbb{R}^{N \times m}}
  \left\{\langle s_*(\bm{\lambda}^T \bm\varphi)\rangle - \sum_{|i|\leq M}\bm{\lambda}_i^T \bm{\hat u}_i\right\},
\end{align}
where $s_*:\mathbb{R}^m\to\mathbb{R}$ is the Legendre transformation of $s$, and $\bm{ \hat\lambda}:=(\bm{\hat{\lambda}}_i)_{|i|\leq M}\in \mathbb{R}^{N \times m}$ are called the dual variables. The solution to \eqref{eq:primalProblem} is then given by
\begin{align}\label{eq:ansatz}
 \mathcal{U}(\bm{\hat u}) = \left( \nabla_{\bm{u}} s \right)^{-1}(\bm{\hat{\lambda}}(\bm{\hat u})^T \bm{\varphi}).
\end{align}
When plugging this ansatz into the original equations \eqref{eq:hyperbolicProblem} and projecting the resulting residual to zero again yields the moment system \eqref{eq:SGMomentSystem}, but with the ansatz \eqref{eq:ansatz} instead of \eqref{eq:SGClosure}.



