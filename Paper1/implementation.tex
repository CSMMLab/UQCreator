\section{Parallelization and Implementation}
\label{sec:parallel}
It remains to discuss the parallelization of the presented algorithms. In order to minimize the parallelization overhead, our goal is to minimize the communication between processors. Note that the dual problem (line 8 in Algorithm~\ref{alg:ad-IPM} and line 5 in Algorithm~\ref{alg:adosIPM}) does not require communication, i.e. it suffices to distribute the spatial cells between processors. In contrast to that, the finite volume update (line 14 in Algorithm~\ref{alg:ad-IPM} and line 8 in Algorithm~\ref{alg:adosIPM}) requires communication, since values at neighboring cells need to be evaluated. Hence, distributing the spatial mesh between processors will yield communication overhead since data needs to be sent whenever a stencil cell lies on a different processor. Therefore, we choose to parallelize the quadrature points, which minimizes the computational time spend on communication. As mentioned in \ref{app:costNumFlux}, we first compute the solution at stencil cells for all quadrature points. I.e. we determine $\bm u^{(j)}_k\in\mathbb{R}^m$ and the corresponding stencil cells for $k = 1,\dots,Q$ by
\begin{align*}
\bm u^{(j-1)}_k := \bm u_{s}(\bm{\lambda}_{j-1}^T\bm{\varphi}_{\ell_1'}(\bm \xi_k)), \enskip \bm u^{(j)}_k := \bm u_{s}(\bm{\lambda}_{j}^T\bm{\varphi}_{\ell_2'}(\bm \xi_k)), \enskip \bm u^{(j+1)}_k := \bm u_{s}(\bm{\lambda}_{j+1}^T\bm{\varphi}_{\ell_3'}(\bm \xi_k)).
\end{align*}
Thus, the finite volume update function \eqref{eq:adaptiveFVUpdate} can be written as
\begin{align}\label{eq:momentUpQuadrature}
\bm{c}_{\ell}^{\bm{\ell}'}&\left(\bm{\lambda}_{1},\bm{\lambda}_2,\bm{\lambda}_3\right)=\sum_{k=1}^Q w_k \left[\bm u^{(j)}_k- \frac{\Delta t}{\Delta x}\left(\bm g( \bm u^{(j)}_k,\bm u^{(j+1)}_k )- \bm g( \bm u^{(j-1)}_k,\bm u^{(j)}_k )\right)\right]\bm{\varphi}_{\ell}(\bm \xi_k)^T.
\end{align}
Instead of distributing the spatial mesh on the different processors, we now distribute the quadrature set, i.e. the sum in \eqref{eq:momentUpQuadrature} can be computed in parallel. Now, after having performed the dual update, the dual variables are send to all processors. With these variables, each processor computes the solution on its portion of the quadrature set and then computes its part of the sum in \eqref{eq:momentUpQuadrature} on all spacial cells. All parts from the different processors are then added together and the full time-updated moments are distributed to all processors. From here, the dual update can again be performed. The standard IPM Algorithm~\ref{alg:IPM} and One-Shot IPM Algorithm~\ref{alg:osIPM} use this parallelization strategy accordingly. Again, we point out that stochastic-Galerkin is a variant of IPM, i.e. all presented techniques for IPM can also be used for SG. The SC algorithm that we use to compare intrusive with non-intrusive methods uses a given deterministic solver as a black box. Here, we distribute the quadrature set between all processors. Note that both, SC and IPM are based on the same deterministic solver, i.e. we use the same deterministic numerical flux $\bm g$. To our best knowledge, this allows a fair comparison of the different intrusive and non-intrusive techniques.
