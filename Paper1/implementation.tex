\section{Parallelization and Implementation}
\label{sec:parallel}
It remains to discuss the parallelization of the presented algorithms. In order to minimize the parallelization overhead, our goal is to minimize the communication between processors. Note that the dual problem (line 8 in Algorithm~\ref{alg:ad-IPM} and line 5 in Algorithm~\ref{alg:adosIPM}) does not require communication, i.e. it suffices to distribute the spatial cells between processors. In contrast to that, the finite volume update (line 14 in Algorithm~\ref{alg:ad-IPM} and line 8 in Algorithm~\ref{alg:adosIPM}) requires communication, since values at neighboring cells need to be evaluated. Hence, distributing the spatial mesh between processors will yield communication overhead since data needs to be sent whenever a stencil cell lies on a different processor. Therefore, we choose to parallelize the quadrature points, which minimizes the computational time spend on communication. As mentioned in \ref{app:costNumFlux}, we first compute the solution at stencil cells for all quadrature points. I.e. we determine $\bm u^{(j)}_k\in\mathbb{R}^m$ and the corresponding stencil cells for $k = 1,\dots,Q$ by
\begin{linenomath*}\begin{align*}
\bm u^{(j-1)}_k := \bm u_{s}(\bm{\lambda}_{j-1}^T\bm{\varphi}_{\ell_1'}(\bm \xi_k)), \enskip \bm u^{(j)}_k := \bm u_{s}(\bm{\lambda}_{j}^T\bm{\varphi}_{\ell_2'}(\bm \xi_k)), \enskip \bm u^{(j+1)}_k := \bm u_{s}(\bm{\lambda}_{j+1}^T\bm{\varphi}_{\ell_3'}(\bm \xi_k)).
\end{align*}\end{linenomath*}
Thus, the finite volume update function \eqref{eq:adaptiveFVUpdate} can be written as
\begin{linenomath*}\begin{align}\label{eq:momentUpQuadrature}
\bm{c}_{\ell}^{\bm{\ell}'}&\left(\bm{\lambda}_{\comment{j-1}},\bm{\lambda}_{\comment{j}},\bm{\lambda}_{\comment{j+1}}\right)=\sum_{k=1}^Q w_k \left[\bm u^{(j)}_k- \frac{\Delta t}{\Delta x}\left(\bm g( \bm u^{(j)}_k,\bm u^{(j+1)}_k )- \bm g( \bm u^{(j-1)}_k,\bm u^{(j)}_k )\right)\right]\bm{\varphi}_{\ell}(\bm \xi_k)^T.
\end{align}\end{linenomath*}
Instead of distributing the spatial mesh on the different processors, we now distribute the quadrature set, i.e. the sum in \eqref{eq:momentUpQuadrature} can be computed in parallel. Now, after having performed the dual update, the dual variables are send to all processors. With these variables, each processor computes the solution on its portion of the quadrature set and then computes its part of the sum in \eqref{eq:momentUpQuadrature} on all spacial cells. All parts from the different processors are then added together and the full time-updated moments are distributed to all processors. From here, the dual update can again be performed. The standard IPM Algorithm~\ref{alg:IPM} and One-Shot IPM Algorithm~\ref{alg:osIPM} use this parallelization strategy accordingly. Again, we point out that stochastic-Galerkin is a variant of IPM, i.e. all presented techniques for IPM can also be used for SG. The SC algorithm that we use to compare intrusive with non-intrusive methods uses a given deterministic solver as a black box. Here, we distribute the quadrature set between all processors. Note that both, SC and IPM are based on the same deterministic solver, i.e. we use the same deterministic numerical flux $\bm g$. To our best knowledge, this allows a fair comparison of the different intrusive and non-intrusive techniques. In the following section, we will study the convergence of the expectation and variance error in pseudo-time. Recording this error is straight forward with intrusive methods, however non-intrusive methods only yield expectation value and variance at the final, steady state solution. Therefore, to record the error for SC, we have implemented a collocation code, which couples all quadrature points in each time step, allowing the computation of the error in pseudo-time. Since this adds additional communication costs, we do not use the run time of this method, but instead make use of the run times from the black-box SC code. Thereby, we are able to record the convergence of expectation values and variances in pseudo-time for the non-intrusive SC method without including additional communication costs.
